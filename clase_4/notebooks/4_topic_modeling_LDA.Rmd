---
title: "Modelado de tópicos Vol. 1. Latent Dirichlet Allocation"
subtitle: "Los diarios de Emilio Renzi (tres tomos)"
output: html_notebook
---

```{r echo=TRUE, results='hide'}
library(tidyverse)
library(topicmodels)
library(tidytext)
```


## Introducción
En la minería de texto, suele ser habiutal disponer de grandes volúmenes de texto (publicaciones de blogs, artículos de noticias, comentarios, etc.) a los cuales nos gustaría poder "dividir" en grupos naturales para poder entenderlos por separado. El modelado de tópicos es un método para la clasificación no supervisada de dichos documentos, similar a la agrupación de datos numéricos, que encuentra grupos naturales de elementos incluso cuando no estamos seguros de lo que estamos buscando.

Hay una gran cantidad de métodos de modelados de tópicos, hoy vamos a ver uno de los más populares: Latent Dirichlet Allocation (LDA). La idea va a ser tratar cada documento como una mixtura, una mezcla de temas y a cada tema como una mixtura de palabras. Esto permite que los documentos se "superpongan" entre sí en términos de contenido, en lugar de estar separados en grupos discretos, de una manera que refleja el uso típico del lenguaje natural.

![](https://www.tidytextmining.com/images/tmwr_0601.png)


## Latent Dirichlet Allocation (LDA). ¿De qué hablan los presidentes en Argentina?

LDA es uno de los algoritmos más comunes para el modelado de tópicos Sin sumergirnos en las matemáticas detrás del modelo, podemos entenderlo como guiado por dos principios.

- Cada documento es una mezcla de temas. Imaginamos que cada documento puede contener palabras de varios temas en proporciones particulares. Por ejemplo, en un modelo de dos temas podríamos decir "El documento 1 es 90% tema A y 10% tema B, mientras que el documento 2 es 30% tema A y 70% tema B".
- Cada tema es una mezcla de palabras. Por ejemplo, podríamos imaginar un modelo de dos temas de noticias estadounidenses, con un tema para "política" y otro para "entretenimiento". Las palabras más comunes en el tema de política pueden ser "presidente", "Congreso" y "gobierno", mientras que el tema de entretenimiento puede estar compuesto por palabras como "películas", "televisión" y "actor". Es importante destacar que las palabras se pueden compartir entre temas; una palabra como "presupuesto" puede aparecer en ambos por igual.

LDA es un método matemático para estimar ambos al mismo tiempo: encontrar la combinación de palabras que está asociada con cada tema, al mismo tiempo que se determina la combinación de temas que describe cada documento. Hay varias implementaciones de este algoritmo y exploraremos una de ellas en profundidad.

Veamos un ejemplo:

```{r}
discursos <- read_csv('../data/discursos.csv')

head(discursos)
```

Disponemos de este dataset que contiene discursos de tres presidentes (Néstor Kirchner, Cristina Fernández y Mauricio Macri). Estamos interesados en poder identificar los temas que toca cada uno pero no estamos interesados en leer todos los discursos. Por eso vamos a usar LDA en este corpus.

Como vemos, el corpus aún no ha sido procesado. Empecemos, entonces, por ahí y podemos, de paso, repasar las diferentes etapas.

## Preprocesamiento

Vamos a quedarnos, por ahora, solamente, con la fecha, el autor y el texto de los discursos y generamos una variable identificadora de los discursos
```{r}
discursos <- discursos %>%
                mutate(id = row_number()) %>%
                select(id, fecha_iso, autor, text) %>%
                mutate(text = stringi::stri_trans_general(text, "Latin-ASCII"))
```




Ahora podemos tokenizarlo:

```{r}
discursos <- discursos %>%
                unnest_tokens(word, text)
```

Hagamos una exploración rápida:

```{r}
discursos %>%
        group_by(word) %>%
        summarise(n=n()) %>%
        arrange(desc(n))
```

Vemos que, claramente, tenemos que hacer una limpieza de stopwords.

```{r}
stop_words <- read_csv('../data/stop_words_complete.csv') %>%
                        mutate(word=stringi::stri_trans_general(word, "Latin-ASCII"))


stop_words <- stop_words %>%
                bind_rows( tibble(word=c('aplausos', 'aplauso'), lexicon=c('custom', 'custom')))
```

```{r}
discursos <- discursos %>%
                anti_join(stop_words)
```

Veamos ahora:

```{r}
discursos %>%
        group_by(word) %>%
        summarise(n=n()) %>%
        arrange(desc(n))
```

#### Pregunta
¿Cómo podríamos identificar las palabras más usadas por los diferentes presidentes?

```{r}
####
```

Ahora sí, estamos en condiciones de avanzar en nuestro modelado de tópicos.

### Modelado de tópicos
Para hacer el modelado de temas como se implementa aquí, necesitamos generar una `DocumentTermMatrix`, un tipo especial de matriz del paquete tm (por supuesto, esto es solo una implementación específica del concepto general de una TFM). 

Las filas corresponden a documentos (textos descriptivos en nuestro caso) y las columnas corresponden a términos (es decir, palabras); es una matriz dispersa y los valores son recuentos de palabras. Primero, generamos nuestra tabla tidy de conteos

```{r}
word_counts <- discursos %>%
        group_by(id, word) %>%
        summarise(n=n()) %>%
        ungroup()
```

Tenemos hasta aquí nuestra estructura de datos habitual: un token por fila y una columna de conteo. Vamos a transformarla ahora a una TFM:

```{r}
disc_dtm <- word_counts %>%
                cast_dtm(id, word, n)

disc_dtm
```


Vemos que este conjunto de datos contiene documentos (cada uno de ellos un discurso) y términos (palabras). Observe que esta matriz de documento-término de ejemplo es (muy cercana a) 100% dispersa, lo que significa que casi todas las entradas en esta matriz son cero. Cada entrada distinta de cero corresponde a una determinada palabra que aparece en un determinado documento.


### Ahora sí, vamos a modelar los tópicos
Ahora usemos el paquete `topicmodels` para estimar un modelo LDA. ¿Cuántos temas le diremos al algoritmo que haga? Esta es una pregunta muy parecida a la de un clustering de k-medias. La respuesta es desilusionadora: no lo sabemos. No queda otra opción que ir probando. Por ahora y a los fines prácticos de esta primera aproximación vamos a probar un modelo extremadamente simple: 2 temas.

```{r}
lda_2 <- LDA(disc_dtm, k=2, control = list(seed = 1234))
lda_2
```

Vemos que entrenar el modelo es simple: una línea de código. Lo difícil viene ahora.

### La interpretación


```{r}
ap_topics <- tidy(lda_2, matrix = "beta")
ap_topics
```

La funciín `tidy`conviritó el modelo a un formato de un tópico-término por fila. Para cada combinación, el modelo calcula la probabilidad de que ese término se genere a partir de ese tópico. Por ejemplo, el término "aaron" tiene un
1,686917
×
10
-
12
 probabilidad de ser generado a partir del tema 1, pero una
3.8959408
×
10
-
5
 probabilidad de ser generado a partir del tema 2.

Podríamos usar el slice_max () de dplyr para encontrar los 10 términos que son más comunes dentro de cada tema. Como marco de datos ordenado, esto se presta bien a una visualización de ggplot2 (Figura 6.2).

```{r}
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)


ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```


```{r}
doc_2_topics <- tidy(lda_2, matrix = "gamma")

doc_2_topics %>%
  rename(id = document) %>%
  mutate(id = as.integer(id)) %>%
  left_join(discursos %>% select(id, autor) %>% unique()) %>%
  group_by(autor, topic) %>%
    summarise(mean = mean(gamma))
```

