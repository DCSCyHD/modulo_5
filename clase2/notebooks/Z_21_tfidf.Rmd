---
title: "Ponderando frecuencias de palabras"
subtitle: "Los diarios de Emilio Renzi (tres tomos)"
output: html_notebook
---

```{r}
library(tidyverse)
library(tidytext)
```

## Introducción
Un problema muy importante en NLP es tratar de entender de qué habla un texto sin leerlo. Esto supone, de alguna forma, intentar cuantificar los contenidos de un texto. Vamos a ir viendo unas cuántas técnicas relativamente sofisticadas vinculadas al modelado de tópicos más adelante, pero por ahora vamos a seguir contando palabras. ¿Podemos a partir del conteo de palabras llegar a tener una idea del contenido de un texto? Como vimos, una primera métrica de la importancia de una palabra es la llamada _term frequency_ (tf): la frecuencia con la que aparece una palabra en un documento.

Sin embargo, hay palabras en un documento que aparecen muchas veces pero que no importante; en castellano, probablemente palabras como "el", "es", "de", etc. Podríamos adoptar el enfoque de agregar palabras como estas a una lista de palabras vacías y eliminarlas antes del análisis, pero es posible que algunas de estas palabras sean más importantes en algunos documentos que en otros. Una lista de palabras vacías no es un enfoque muy sofisticado para ajustar la frecuencia de los términos para palabras de uso común.

Podemos analizar la "informatividad" de un término observando la _inverse document frequency_ (idf). Esta métrica disminuye el peso de las palabras de uso común y aumenta el peso de las palabras que no se usan mucho en una colección de documentos. 

Finalmente, podemos calcular la tf-idf que combina ambas métricas multiplicándolas: es la frecuencia de cada término, ajustada por la importancia que tiene en el corpus.


## Term frequency en los diarios de Renzi

Vamos a arrancar con los 3 tomos de los diarios de Renzi. Examinemos primero tf y luego tf-idf. Podemos comenzar simplemente usando verbos dplyr como `group_by()` y `join()`. ¿Cuáles son las palabras más utilizadas en los diarios? (También vamos a calcular el total de palabras en cada novela aquí, para uso posterior).


```{r}
renzi <- read_csv('../data/renzi.csv') %>%
        mutate(tomo = case_when(
                tomo == '1_diarios_renzi_años_de_formacion.txt' ~ 'I-Años de formación',
                tomo == '2_diarios_renzi_los_años_felices.txt' ~ 'II-Los años felices',
                tomo == '3_diarios_renzi_un_dia_en_la_vida.txt' ~ 'III-Un día en la vida',
        ))
```

```{r}
book_words <- renzi %>%
        mutate(entry_number = row_number()) %>%
        unnest_tokens(output = word, 
                      input = entry) %>%
        group_by(tomo, word) %>%
        summarise(n = n()) %>%
        arrange(desc(n)) %>%
        ungroup()

total_words <- book_words %>% 
  group_by(tomo) %>% 
  summarize(total = sum(n))

book_words <- book_words %>%
                left_join(total_words) %>%
                ungroup()

head(book_words)
```

Hay una fila en este `tibble` book_words para cada combinación de tomo y palabras; 

- `n` es el número de veces que se usa esa palabra en ese tomo 
- `total` es el total de palabras en ese libro. 

Vemos las palabras que eliminaríamos mediante una lista como stopwords: "de", "la", "que", etc.

En el siguiente gráfico veamos la distribución porcentual (`n / total`) para cada tomo, el número de veces que aparece una palabra en un tomo dividido por el número total de términos (palabras) en ese tomo. Hemos cálculado la _term frequency_.

```{r}
book_words %>%
        mutate(tf = n/total) %>%
        ggplot(aes(tf, fill = tomo)) +
                geom_histogram(show.legend = FALSE) +
                xlim(NA, 0.00015) +
                facet_wrap(~tomo,  scales = "free_y") +
                theme_minimal()

```

Hay colas muy largas a la derecha para esos tomos (son muy raras) que no hemos mostrado en estos tramas. En general, los gráficos son similares en las novelas. Pocas palabras que ocurren mucho y muchas poco.


## Ley de Zipf
Las distribuciones como las anteriores son bastante típicas en el lenguaje. De hecho, esos tipos de distribuciones de cola larga son tan comunes en cualquier corpus (como un libro, una gran cantidad de texto de un sitio web o palabras habladas) que la relación entre la frecuencia con la que se usa una palabra y su rango ha sido objeto de estudio; una versión clásica de esta relación se llama [ley de Zipf](https://en.wikipedia.org/wiki/Zipf%27s_law), en honor a George Zipf, un lingüista estadounidense del siglo XX.

La idea es que la frecuencia con que aparece una palabra es inversamente proporcional a su rango. Más precisamente que para un cierto idioma la frecuencia de aparición de las diferentes palabras puede ser aproximada mediante la siguiente función:

$$P_{n} = \frac{1}{n^a} $$
donde $P_{n}$ representa la frecuencia de la n-ésima palabra más frecuente y el exponente $a$ es un número real positivo, generalmente superior a 1.1. 

```{r}
freq_by_rank <- book_words %>% 
  group_by(tomo) %>% 
  mutate(rank = row_number(), 
         `tf` = n/total) %>%
        ungroup()

freq_by_rank
```

La columna de `rank` nos informa el orden de cada palabra dentro de la tabla de frecuencia; la tabla ya estaba ordenada por `n`, por lo que podríamos usar `row_number()` para encontrar el rango. 

Luego, podemos calcular la frecuencia del término de la misma manera que lo hicimos antes. La ley de Zipf a menudo se visualiza trazando el rango en el eje x y la frecuencia del término en el eje y, en escalas logarítmicas. Graficando de esta manera, una relación inversamente proporcional tendrá una pendiente negativa constante.

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `tf`, color = tomo)) +
        geom_line(size = 1.1, alpha = 0.8) +
        scale_x_log10() +
        scale_y_log10() +
        theme_minimal()
```

Observemos que el gráfico anterior está en coordenadas log-log. Los tres tomos de los diarios de REnzi son similares: la relación entre rango y frecuencia tiene una pendiente negativa.


### Calculando todas las métricas: la función `bind_tfi_idf()`
Repitámoslo una vez más: la idea de tf-idf es encontrar las palabras importantes para el contenido de cada documento disminuyendo el peso de las palabras de uso común y aumentando el peso de las palabras que no se usan mucho en una colección o corpus de documentos. El cálculo de tf-idf intenta encontrar las palabras que son importantes (es decir, comunes) en un texto, pero informativas (no demasiado comunes).

La función `bind_tf_idf()` en el paquete `tidytext` toma un dataset de texto tidy como entrada con una fila por token (término), por documento. Una columna (`word` aquí) contiene los términos / tokens, otra columna contiene los documentos (`tomo` en este caso) y la última columna necesaria contiene los recuentos, cuántas veces cada documento contiene cada término (`n` en este ejemplo). Calculamos un total para cada tomo para nuestras exploraciones en las secciones anteriores, pero no es necesario para la función `bind_tf_idf ()`; la tabla solo debe contener todas las palabras de cada documento.

```{r}
book_tf_idf <- book_words %>%
  bind_tf_idf(word, tomo, n)

head(book_tf_idf)
```

Notar que `idf` y, por lo tanto, `tf-idf` son cero para estas palabras extremadamente comunes (que en el ejercicio anterior, habíamos eliminado como stopwords). Todas estas son palabras que aparecen en los tres tomos de los diarios, por lo que el término `idf` (que entonces será el logaritmo natural de 1) es cero. La frecuencia inversa del documento (y por tanto `tf-idf`) es muy baja (cercana a cero) para las palabras que aparecen en muchos de los documentos de una colección; así es como este enfoque reduce el peso de las palabras comunes. La `idf` será un número mayor para las palabras que aparecen en una menor cantidad de documentos de la colección.

Ordenemos la tabla de forma diferente para identificar las palabras más importantes:

```{r}
book_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

Aquí vemos que varios nombres y apodos (carola, schucler, toto, etc.) son importantes en los tres tomos, aunque no se repiten a lo largo de los tomos.

Visualicemos estos términos. Vamos a quedarnos (para cada tomo) con los principales 20 términos según tf-idf. 
```{r}
library(forcats)

book_tf_idf %>%
  group_by(tomo) %>%
        slice_max(tf_idf, n = 20) %>%
        ungroup() %>%
        ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = tomo)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(~tomo, ncol = 2, scales = "free") +
        labs(x = "tf-idf", y = NULL) +
        theme_minimal()
```

Es muy interesante ver como los nombres aparecen pero son diferentes en cada etapa de la vida de Renzi. A su vez, en los "años de formación" se ve como la vida académica de Renzi (o sea, Piglia) tenía su importancia: términos como "concursos", "sinóptico" o "monografía" rankean entre los más imporantes. 

A medida que avanza la vida de Piglia/Renzi vemos como otros personajes como "Toto Schmucler" toma importancia en su vida y es en el segundo tomo en el que viaja a China y por eso palabras como "china" o "mao" resultan relevantes.

Finalmente, en el período final toman importancia los años "1976" y "1977", los años de encierro de la dictadura. También su migración a Estados Unidos aparece capturada por el término "princeton", la universidad en la que enseñó muchos años. Es la época en que termina de escribir y publica la novela Respiración Artificial en la que "arocena" es un personaje importante (el censor y el espía que lee las cartas).

### Actividad
A partir del corpus de textos de Marx y Engels trabajado la clase pasada, identificar y graficar las 20 principales palabras en las cartas, las notas y los libros.

```{r}
###
```


### Resumen
El uso de tf-idf nos permite encontrar palabras que son características de un documento dentro de una colección de documentos, ya sea que ese documento sea una novela, un diario un texto físico o una página web. Explorar la frecuencia de los términos por sí solo puede darnos una idea de cómo se usa el lenguaje en una colección de lenguaje natural, y los verbos dplyr como `count()` y `rank()` nos brindan herramientas para razonar sobre la frecuencia de los términos. El paquete `tidytext` utiliza una implementación de tf-idf consistente con los principios de tidy data que nos permite ver cómo diferentes palabras son importantes en los documentos dentro de una colección o corpus de documentos.
